{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Multiclass margin classifier\n",
    "============================\n",
    "\n",
    ".. meta::\n",
    "    :property=\"og:description\": Using PyTorch to implement a multiclass\n",
    "        quantum variational classifier on MNIST data.\n",
    "    :property=\"og:image\": https://pennylane.ai/qml/_images/margin_2.png\n",
    "\n",
    ".. related::\n",
    "\n",
    "   tutorial_variational_classifier Variational quantum classifier\n",
    "   tutorial_data_reuploading_classifier Data-reuploading classifier\n",
    "\n",
    "*Author: PennyLane dev team. Posted: 9 Apr 2020. Last updated: 28 Jan 2021.*\n",
    "\n",
    "In this tutorial, we show how to use the PyTorch interface for PennyLane\n",
    "to implement a multiclass variational classifier. We consider the iris database\n",
    "from UCI, which has 4 features and 3 classes. We use multiple one-vs-all\n",
    "classifiers with a margin loss (see `Multiclass Linear SVM\n",
    "<http://cs231n.github.io/linear-classify/>`__) to classify data. Each classifier is implemented\n",
    "on an individual variational circuit, whose architecture is inspired by\n",
    "`Farhi and Neven (2018) <https://arxiv.org/abs/1802.06002>`__ as well as\n",
    "`Schuld et al. (2018) <https://arxiv.org/abs/1804.00633>`__.\n",
    "\n",
    "|\n",
    "\n",
    ".. figure:: ../demonstrations/multiclass_classification/margin_2.png\n",
    "    :align: center\n",
    "    :width: 50%\n",
    "    :target: javascript:void(0)\n",
    "\n",
    "|\n",
    "\n",
    "\n",
    "Initial Setup\n",
    "~~~~~~~~~~~~~\n",
    "\n",
    "We import PennyLane, the PennyLane-provided version of NumPy,\n",
    "relevant torch modules, and define the constants that will\n",
    "be used in this tutorial.\n",
    "\n",
    "Our feature size is 4, and we will use amplitude embedding.\n",
    "This means that each possible amplitude (in the computational basis) will\n",
    "correspond to a single feature. With 2 qubits (wires), there are\n",
    "4 possible states, and as such, we can encode a feature vector\n",
    "of size 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from pennylane import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_classes = 3\n",
    "margin = 0.15\n",
    "feature_size = 3\n",
    "batch_size = 10\n",
    "lr_adam = 0.01\n",
    "train_split = 0.75\n",
    "# the number of the required qubits is calculated from the number of features\n",
    "num_qubits = int(np.ceil(np.log2(feature_size)))\n",
    "num_layers = 6\n",
    "total_iterations = 100\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantum Circuit\n",
    "~~~~~~~~~~~~~~~\n",
    "\n",
    "We first create the layer that will be repeated in our variational quantum\n",
    "circuits. It consists of rotation gates for each qubit, followed by\n",
    "entangling/CNOT gates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(W):\n",
    "    for i in range(num_qubits):\n",
    "        qml.Rot(W[i, 0], W[i, 1], W[i, 2], wires=i)\n",
    "    for j in range(num_qubits - 1):\n",
    "        qml.CNOT(wires=[j, j + 1])\n",
    "    if num_qubits >= 2:\n",
    "        # Apply additional CNOT to entangle the last with the first qubit\n",
    "        qml.CNOT(wires=[num_qubits - 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the quantum nodes that will be used. As we are implementing our\n",
    "multiclass classifier as multiple one-vs-all classifiers, we will use 3 QNodes,\n",
    "each representing one such classifier. That is, ``circuit1`` classifies if a\n",
    "sample belongs to class 1 or not, and so on. The circuit architecture for all\n",
    "nodes are the same. We use the PyTorch interface for the QNodes.\n",
    "Data is embedded in each circuit using amplitude embedding.\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>For demonstration purposes we are using a very simple circuit here. \n",
    "    You may find that other choices, for example more \n",
    "    elaborate measurements, increase  the power of the classifier.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit(weights, feat=None):\n",
    "    qml.templates.embeddings.AmplitudeEmbedding(feat, range(num_qubits), pad=0.0, normalize=True)\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "qnodes = []\n",
    "for iq in range(num_classes):\n",
    "    qnode = qml.QNode(circuit, dev, interface=\"torch\")\n",
    "    qnodes.append(qnode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational quantum circuit is parametrized by the weights. We use a\n",
    "classical bias term that is applied after processing the quantum circuit's\n",
    "output. Both variational circuit weights and classical bias term are optimized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(q_circuit, params, feat):\n",
    "    weights = params[0]\n",
    "    bias = params[1]\n",
    "    return q_circuit(weights, feat=feat) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function\n",
    "~~~~~~~~~~~~~\n",
    "\n",
    "Implementing multiclass classifiers as a number of one-vs-all classifiers\n",
    "generally evokes using the margin loss. The output of the $i$ th classifier, $c_i$\n",
    "on input $x$ is interpreted as a score, $s_i$ between [-1,1].\n",
    "More concretely, we have:\n",
    "\n",
    "\\begin{align}s_i = c_i(x; \\theta)\\end{align}\n",
    "\n",
    "The multiclass margin loss attempts to ensure that the score for the correct\n",
    "class is higher than that of incorrect classes by some margin. For a sample $(x,y)$\n",
    "where $y$ denotes the class label, we can analytically express the mutliclass\n",
    "loss on this sample as:\n",
    "\n",
    "\\begin{align}L(x,y) = \\sum_{j \\ne y}{\\max{\\left(0, s_j - s_y + \\Delta)\\right)}}\\end{align}\n",
    "\n",
    "where $\\Delta$ denotes the margin. The margin parameter is chosen as a hyperparameter.\n",
    "For more information, see `Multiclass Linear SVM <http://cs231n.github.io/linear-classify/>`__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_svm_loss(q_circuits, all_params, feature_vecs, true_labels):\n",
    "    loss = 0\n",
    "    num_samples = len(true_labels)\n",
    "    for i, feature_vec in enumerate(feature_vecs):\n",
    "        # Compute the score given to this sample by the classifier corresponding to the\n",
    "        # true label. So for a true label of 1, get the score computed by classifer 1,\n",
    "        # which distinguishes between \"class 1\" or \"not class 1\".\n",
    "        s_true = variational_classifier(\n",
    "            q_circuits[int(true_labels[i])],\n",
    "            (all_params[0][int(true_labels[i])], all_params[1][int(true_labels[i])]),\n",
    "            feature_vec,\n",
    "        )\n",
    "        s_true = s_true.float()\n",
    "        li = 0\n",
    "\n",
    "        # Get the scores computed for this sample by the other classifiers\n",
    "        for j in range(num_classes):\n",
    "            if j != int(true_labels[i]):\n",
    "                s_j = variational_classifier(\n",
    "                    q_circuits[j], (all_params[0][j], all_params[1][j]), feature_vec\n",
    "                )\n",
    "                s_j = s_j.float()\n",
    "                li += torch.max(torch.zeros(1).float(), s_j - s_true + margin)\n",
    "        loss += li\n",
    "\n",
    "    return loss / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Function\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Next, we use the learned models to classify our samples. For a given sample,\n",
    "compute the score given to it by classifier $i$, which quantifies how likely it is that\n",
    "this sample belongs to class $i$. For each sample, return the class with the highest score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(q_circuits, all_params, feature_vecs, labels):\n",
    "    predicted_labels = []\n",
    "    for i, feature_vec in enumerate(feature_vecs):\n",
    "        scores = np.zeros(num_classes)\n",
    "        for c in range(num_classes):\n",
    "            score = variational_classifier(\n",
    "                q_circuits[c], (all_params[0][c], all_params[1][c]), feature_vec\n",
    "            )\n",
    "            scores[c] = float(score)\n",
    "        pred_class = np.argmax(scores)\n",
    "        predicted_labels.append(pred_class)\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def accuracy(labels, hard_predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, hard_predictions):\n",
    "        if torch.abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / labels.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Processing\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Now we load in the iris dataset and normalize the features so that the sum of the feature\n",
    "elements squared is 1 ($\\ell_2$ norm is 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First X sample, original  : tensor([5.1000, 3.5000, 1.4000], dtype=torch.float64)\n",
      "First X sample, normalized: tensor([0.8042, 0.5519, 0.2208], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8042, 0.5519, 0.2208],\n",
       "        [0.8286, 0.5073, 0.2367],\n",
       "        [0.8058, 0.5486, 0.2229],\n",
       "        [0.8005, 0.5395, 0.2610],\n",
       "        [0.7914, 0.5698, 0.2216],\n",
       "        [0.7855, 0.5673, 0.2473],\n",
       "        [0.7811, 0.5774, 0.2377],\n",
       "        [0.8026, 0.5458, 0.2408],\n",
       "        [0.8070, 0.5319, 0.2568],\n",
       "        [0.8181, 0.5176, 0.2505],\n",
       "        [0.8041, 0.5510, 0.2234],\n",
       "        [0.7874, 0.5578, 0.2625],\n",
       "        [0.8232, 0.5145, 0.2401],\n",
       "        [0.8027, 0.5600, 0.2053],\n",
       "        [0.8115, 0.5597, 0.1679],\n",
       "        [0.7750, 0.5982, 0.2039],\n",
       "        [0.7957, 0.5746, 0.1915],\n",
       "        [0.8042, 0.5519, 0.2208],\n",
       "        [0.8076, 0.5384, 0.2409],\n",
       "        [0.7805, 0.5815, 0.2296],\n",
       "        [0.8177, 0.5149, 0.2574],\n",
       "        [0.7874, 0.5713, 0.2316],\n",
       "        [0.7762, 0.6075, 0.1687],\n",
       "        [0.8085, 0.5232, 0.2695],\n",
       "        [0.7765, 0.5500, 0.3074],\n",
       "        [0.8269, 0.4962, 0.2646],\n",
       "        [0.7994, 0.5436, 0.2558],\n",
       "        [0.8068, 0.5430, 0.2327],\n",
       "        [0.8165, 0.5339, 0.2198],\n",
       "        [0.7957, 0.5418, 0.2709],\n",
       "        [0.8089, 0.5224, 0.2696],\n",
       "        [0.8238, 0.5187, 0.2288],\n",
       "        [0.7659, 0.6039, 0.2209],\n",
       "        [0.7790, 0.5949, 0.1983],\n",
       "        [0.8181, 0.5176, 0.2505],\n",
       "        [0.8256, 0.5284, 0.1981],\n",
       "        [0.8274, 0.5265, 0.1956],\n",
       "        [0.8181, 0.5176, 0.2505],\n",
       "        [0.8027, 0.5473, 0.2371],\n",
       "        [0.8082, 0.5388, 0.2377],\n",
       "        [0.8013, 0.5609, 0.2083],\n",
       "        [0.8624, 0.4408, 0.2491],\n",
       "        [0.7866, 0.5721, 0.2324],\n",
       "        [0.7925, 0.5547, 0.2536],\n",
       "        [0.7683, 0.5725, 0.2862],\n",
       "        [0.8232, 0.5145, 0.2401],\n",
       "        [0.7777, 0.5794, 0.2440],\n",
       "        [0.7964, 0.5540, 0.2424],\n",
       "        [0.7987, 0.5576, 0.2261],\n",
       "        [0.8127, 0.5364, 0.2276],\n",
       "        [0.7762, 0.3548, 0.5212],\n",
       "        [0.7571, 0.3786, 0.5324],\n",
       "        [0.7656, 0.3440, 0.5437],\n",
       "        [0.7661, 0.3204, 0.5572],\n",
       "        [0.7701, 0.3317, 0.5450],\n",
       "        [0.7323, 0.3597, 0.5782],\n",
       "        [0.7390, 0.3871, 0.5513],\n",
       "        [0.7684, 0.3764, 0.5175],\n",
       "        [0.7718, 0.3391, 0.5379],\n",
       "        [0.7388, 0.3836, 0.5541],\n",
       "        [0.7785, 0.3114, 0.5449],\n",
       "        [0.7526, 0.3827, 0.5358],\n",
       "        [0.7958, 0.2918, 0.5306],\n",
       "        [0.7413, 0.3524, 0.5712],\n",
       "        [0.7712, 0.3994, 0.4958],\n",
       "        [0.7796, 0.3607, 0.5120],\n",
       "        [0.7193, 0.3853, 0.5780],\n",
       "        [0.7633, 0.3553, 0.5396],\n",
       "        [0.7779, 0.2760, 0.5646],\n",
       "        [0.7705, 0.3440, 0.5366],\n",
       "        [0.7150, 0.3878, 0.5817],\n",
       "        [0.7807, 0.3584, 0.5119],\n",
       "        [0.7533, 0.2989, 0.5859],\n",
       "        [0.7445, 0.3417, 0.5736],\n",
       "        [0.7769, 0.3520, 0.5220],\n",
       "        [0.7782, 0.3537, 0.5188],\n",
       "        [0.7743, 0.3188, 0.5466],\n",
       "        [0.7543, 0.3378, 0.5629],\n",
       "        [0.7462, 0.3606, 0.5596],\n",
       "        [0.7943, 0.3623, 0.4877],\n",
       "        [0.7743, 0.3379, 0.5350],\n",
       "        [0.7802, 0.3404, 0.5248],\n",
       "        [0.7741, 0.3604, 0.5205],\n",
       "        [0.7207, 0.3243, 0.6126],\n",
       "        [0.7066, 0.3925, 0.5888],\n",
       "        [0.7286, 0.4129, 0.5465],\n",
       "        [0.7656, 0.3542, 0.5370],\n",
       "        [0.7854, 0.2867, 0.5485],\n",
       "        [0.7406, 0.3968, 0.5423],\n",
       "        [0.7591, 0.3450, 0.5521],\n",
       "        [0.7326, 0.3463, 0.5860],\n",
       "        [0.7432, 0.3655, 0.5604],\n",
       "        [0.7723, 0.3462, 0.5326],\n",
       "        [0.7792, 0.3584, 0.5142],\n",
       "        [0.7464, 0.3599, 0.5598],\n",
       "        [0.7413, 0.3901, 0.5462],\n",
       "        [0.7450, 0.3790, 0.5489],\n",
       "        [0.7670, 0.3588, 0.5320],\n",
       "        [0.7940, 0.3892, 0.4670],\n",
       "        [0.7541, 0.3704, 0.5424],\n",
       "        [0.6771, 0.3547, 0.6448],\n",
       "        [0.7089, 0.3300, 0.6233],\n",
       "        [0.7315, 0.3091, 0.6078],\n",
       "        [0.7068, 0.3253, 0.6282],\n",
       "        [0.7055, 0.3256, 0.6295],\n",
       "        [0.7236, 0.2856, 0.6284],\n",
       "        [0.6895, 0.3518, 0.6332],\n",
       "        [0.7250, 0.2880, 0.6257],\n",
       "        [0.7277, 0.2715, 0.6299],\n",
       "        [0.7129, 0.3564, 0.6040],\n",
       "        [0.7336, 0.3612, 0.5756],\n",
       "        [0.7325, 0.3090, 0.6066],\n",
       "        [0.7354, 0.3245, 0.5948],\n",
       "        [0.7140, 0.3131, 0.6263],\n",
       "        [0.7060, 0.3408, 0.6208],\n",
       "        [0.7187, 0.3594, 0.5952],\n",
       "        [0.7200, 0.3323, 0.6092],\n",
       "        [0.7070, 0.3489, 0.6152],\n",
       "        [0.7222, 0.2439, 0.6472],\n",
       "        [0.7394, 0.2711, 0.6162],\n",
       "        [0.7260, 0.3367, 0.5997],\n",
       "        [0.7044, 0.3522, 0.6163],\n",
       "        [0.7275, 0.2646, 0.6330],\n",
       "        [0.7477, 0.3205, 0.5816],\n",
       "        [0.7131, 0.3512, 0.6067],\n",
       "        [0.7270, 0.3231, 0.6058],\n",
       "        [0.7447, 0.3363, 0.5765],\n",
       "        [0.7279, 0.3580, 0.5847],\n",
       "        [0.7148, 0.3127, 0.6255],\n",
       "        [0.7407, 0.3086, 0.5967],\n",
       "        [0.7407, 0.2803, 0.6106],\n",
       "        [0.7278, 0.3501, 0.5896],\n",
       "        [0.7148, 0.3127, 0.6255],\n",
       "        [0.7346, 0.3265, 0.5947],\n",
       "        [0.7028, 0.2996, 0.6452],\n",
       "        [0.7497, 0.2921, 0.5939],\n",
       "        [0.6931, 0.3741, 0.6161],\n",
       "        [0.7119, 0.3448, 0.6118],\n",
       "        [0.7274, 0.3637, 0.5819],\n",
       "        [0.7424, 0.3335, 0.5810],\n",
       "        [0.7231, 0.3346, 0.6044],\n",
       "        [0.7563, 0.3398, 0.5590],\n",
       "        [0.7089, 0.3300, 0.6233],\n",
       "        [0.7117, 0.3349, 0.6175],\n",
       "        [0.7131, 0.3512, 0.6067],\n",
       "        [0.7448, 0.3335, 0.5780],\n",
       "        [0.7480, 0.2968, 0.5936],\n",
       "        [0.7346, 0.3391, 0.5877],\n",
       "        [0.6968, 0.3821, 0.6069],\n",
       "        [0.7061, 0.3590, 0.6104]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/multiclass_classification/iris.csv\", delimiter=\",\")\n",
    "X = torch.tensor(data[:, 0:feature_size])\n",
    "print(\"First X sample, original  :\", X[0])\n",
    "\n",
    "# normalize each input\n",
    "normalization = torch.sqrt(torch.sum(X ** 2, dim=1))\n",
    "X_norm = X / normalization.reshape(len(X), 1)\n",
    "print(\"First X sample, normalized:\", X_norm[0])\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16179, 3)\n",
      "(14308, 3)\n",
      "(15288, 3)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "count = 1\n",
    "index = 1\n",
    "with open('activity.csv', 'r') as file:\n",
    "# with open('tranquil.csv', 'r') as file:\n",
    "# with open('piano.csv', 'r') as file:\n",
    "# with open('active.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        count += 1\n",
    "        if count % 2 == 0 and count > 2:\n",
    "            x = [int(float(row[6])), int(float(row[7])), int(float(row[8]))]\n",
    "            data.append(x / nump.linalg.norm(x))\n",
    "#             data.append(int(row[5]))\n",
    "#             data.append(np.array([int(float(row[6])), int(float(row[7])), int(float(row[8]))]))\n",
    "            \n",
    "\n",
    "print(np.shape(data))\n",
    "\n",
    "data_2 = []\n",
    "count = 1\n",
    "index = 1\n",
    "with open('tranquil.csv', 'r') as file:\n",
    "# with open('tranquil.csv', 'r') as file:\n",
    "# with open('piano.csv', 'r') as file:\n",
    "# with open('active.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        count += 1\n",
    "        if count % 2 == 0 and count > 2:\n",
    "            x = [int(float(row[6])), int(float(row[7])), int(float(row[8]))]\n",
    "            data_2.append(x / nump.linalg.norm(x))\n",
    "#             data_2.append(np.array([int(float(row[6])), int(float(row[7])), int(float(row[8]))]))\n",
    "\n",
    "print(np.shape(data_2))\n",
    "\n",
    "data_3 = []\n",
    "count = 1\n",
    "index = 1\n",
    "with open('working.csv', 'r') as file:\n",
    "# with open('tranquil.csv', 'r') as file:\n",
    "# with open('piano.csv', 'r') as file:\n",
    "# with open('active.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        count += 1\n",
    "        if count % 2 == 0 and count > 2:\n",
    "            x = [int(float(row[6])), int(float(row[7])), int(float(row[8]))]\n",
    "            data_3.append(x / nump.linalg.norm(x))\n",
    "#             data_3.append([int(float(row[6])), int(float(row[7])), int(float(row[8]))])\n",
    "\n",
    "print(np.shape(data_3))\n",
    "\n",
    "#### next step is to normalize\n",
    "# x = data[10]\n",
    "x = [int(float(row[6])), int(float(row[7])), int(float(row[8]))]\n",
    "data_3.append(x / np.linalg.norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51256251, 0.68888267, 0.51256251])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5126, 0.6889, 0.5126],\n",
       "        [0.7852, 0.6106, 0.1028],\n",
       "        [0.9534, 0.2968, 0.0550],\n",
       "        ...,\n",
       "        [0.8452, 0.4742, 0.2466],\n",
       "        [0.8461, 0.4621, 0.2658],\n",
       "        [0.8461, 0.4621, 0.2658]], dtype=torch.float64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "# X = np.append([data, data_2, data_3])\n",
    "for i in range(len(data)):\n",
    "    X.append(np.array(data[i]))\n",
    "\n",
    "for i in range(len(data_2)):\n",
    "    X.append(np.array(data_2[i]))\n",
    "\n",
    "#     X = np.append(data_2[i], X)\n",
    "\n",
    "for i in range(len(data_3)):\n",
    "    X.append(np.array(data_3[i]))\n",
    "\n",
    "#     X = np.append(data_3[i], X)\n",
    "\n",
    "# X = np.array(X, requires_grad=False)\n",
    "# print(X)\n",
    "\n",
    "# X = np.append([data, data_2, data_3])\n",
    "labels = 0*np.ones(len(data), dtype=np.int8).numpy()\n",
    "for i in range(len(labels)):\n",
    "    Y.append(labels[i])\n",
    "\n",
    "labels_2 = 1*np.ones(len(data_2), dtype=np.int8)\n",
    "for i in range(len(labels_2)):\n",
    "    Y.append(labels_2[i])\n",
    "\n",
    "#     X = np.append(data_2[i], X)\n",
    "\n",
    "labels_3 = 2*np.ones(len(data_3), dtype=np.int8)\n",
    "for i in range(len(data_3)):\n",
    "    Y.append(labels_3[i])\n",
    "    \n",
    "print(type(X))\n",
    "X = torch.tensor(np.array(X))\n",
    "Y = torch.tensor(Y)\n",
    "# print(Y)\n",
    "X\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5126, 0.6889, 0.5126],\n",
       "        [0.7852, 0.6106, 0.1028],\n",
       "        [0.9534, 0.2968, 0.0550],\n",
       "        ...,\n",
       "        [0.8452, 0.4742, 0.2466],\n",
       "        [0.8461, 0.4621, 0.2658],\n",
       "        [0.8461, 0.4621, 0.2658]], dtype=torch.float64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First X sample, normalized: tensor([0.5126, 0.6889, 0.5126], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# print(\"First X sample, original  :\", X_sample[0])\n",
    "print(\"First X sample, normalized:\", X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "First X sample, original  : tensor([5.1000, 3.5000, 1.4000], dtype=torch.float64)\n",
      "First X sample, normalized: tensor([0.8042, 0.5519, 0.2208], dtype=torch.float64)\n",
      "First X sample, normalized: torch.Size([150, 3])\n",
      "First X sample, normalized: tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/multiclass_classification/iris.csv\", delimiter=\",\")\n",
    "Y_data = torch.tensor(data[:, -1])\n",
    "print(type(X_sample))\n",
    "X_sample = torch.tensor(data[:, 0:feature_size])\n",
    "print(\"First X sample, original  :\", X_sample[0])\n",
    "\n",
    "# normalize each input\n",
    "normalization = torch.sqrt(torch.sum(X_sample ** 2, dim=1))\n",
    "X_norm_smaple = X_sample / normalization.reshape(len(X_sample), 1)\n",
    "print(\"First X sample, normalized:\", X_norm[0])\n",
    "print(\"First X sample, normalized:\", np.shape(X_norm))\n",
    "\n",
    "print(\"First X sample, normalized:\", Y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data():\n",
    "#     data = np.loadtxt(\"data/multiclass_classification/iris.csv\", delimiter=\",\")\n",
    "#     X = torch.tensor(data[:, 0:feature_size])\n",
    "#     print(\"First X sample, original  :\", X[0])\n",
    "\n",
    "#     # normalize each input\n",
    "#     normalization = torch.sqrt(torch.sum(X ** 2, dim=1))\n",
    "#     X_norm = X / normalization.reshape(len(X), 1)\n",
    "#     print(\"First X sample, normalized:\", X_norm[0])\n",
    "    load()\n",
    "    \n",
    "#     Y = torch.tensor(data[:, -1])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Create a train and test split.\n",
    "def split_data(feature_vecs, Y):\n",
    "    num_data = len(Y)\n",
    "    num_train = int(train_split * num_data)\n",
    "    index = np.random.permutation(range(num_data))\n",
    "    print(index)\n",
    "    print(feature_vecs)\n",
    "    feat_vecs_train = feature_vecs[index[:num_train]]\n",
    "    Y_train = Y[index[:num_train]]\n",
    "    feat_vecs_test = feature_vecs[index[num_train:]]\n",
    "    Y_test = Y[index[num_train:]]\n",
    "    return feat_vecs_train, feat_vecs_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Procedure\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "In the training procedure, we begin by first initializing randomly the parameters\n",
    "we wish to learn (variational circuit weights and classical bias). As these are\n",
    "the variables we wish to optimize, we set the ``requires_grad`` flag to ``True``. We use\n",
    "minibatch training---the average loss for a batch of samples is computed, and the\n",
    "optimization step is based on this. Total training time with the default parameters\n",
    "is roughly 15 minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34944 20588 24356 ... 13798 12853 11981]\n",
      "tensor([[0.5126, 0.6889, 0.5126],\n",
      "        [0.7852, 0.6106, 0.1028],\n",
      "        [0.9534, 0.2968, 0.0550],\n",
      "        ...,\n",
      "        [0.8452, 0.4742, 0.2466],\n",
      "        [0.8461, 0.4621, 0.2658],\n",
      "        [0.8461, 0.4621, 0.2658]], dtype=torch.float64)\n",
      "Num params:  111\n",
      "DATA_PREPARED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-65bb42d7de6d>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_bias = [torch.tensor(0.1 * torch.ones(1), requires_grad=True) for i in range(num_classes)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 0.2208004 | Acc train: 0.4550274 | Acc test: 0.4556099 \n",
      "DATA_PREPARED\n"
     ]
    }
   ],
   "source": [
    "def training(features, Y):\n",
    "    num_data = Y.shape[0]\n",
    "    feat_vecs_train, feat_vecs_test, Y_train, Y_test = split_data(features, Y)\n",
    "    num_train = Y_train.shape[0]\n",
    "    q_circuits = qnodes\n",
    "\n",
    "    # Initialize the parameters\n",
    "    all_weights = [\n",
    "        Variable(0.1 * torch.randn(num_layers, num_qubits, 3), requires_grad=True)\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "    all_bias = [torch.tensor(0.1 * torch.ones(1), requires_grad=True) for i in range(num_classes)]\n",
    "    optimizer = optim.Adam(all_weights + all_bias, lr=lr_adam)\n",
    "    params = (all_weights, all_bias)\n",
    "#     print(params)\n",
    "    print(\"Num params: \", 3 * num_layers * num_qubits * 3 + 3)\n",
    "\n",
    "    costs, train_acc, test_acc = [], [], []\n",
    "\n",
    "    # train the variational classifier\n",
    "    for it in range(total_iterations):\n",
    "        batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "#         print(feat_vecs_train[0])\n",
    "#         print(batch_index)\n",
    "#         print(feat_vecs_train[0])\n",
    "#         feat_vecs_train_np = np.array(feat_vecs_train)\n",
    "#         Y_train_np = np.array(Y_train)\n",
    "        \n",
    "        feat_vecs_train_batch = []\n",
    "        for i in range(len(batch_index)):\n",
    "            feat_vecs_train_batch.append(feat_vecs_train[i])\n",
    "#         = feat_vecs_train_np[batch_index]\n",
    "        \n",
    "        Y_train_batch = []\n",
    "        for i in range(len(batch_index)):\n",
    "            Y_train_batch.append(Y_train[i])\n",
    "            \n",
    "        print('DATA_PREPARED')\n",
    "#         Y_train_batch = Y_train_np[batch_index]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        curr_cost = multiclass_svm_loss(q_circuits, params, feat_vecs_train_batch, Y_train_batch)\n",
    "        curr_cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute predictions on train and validation set\n",
    "        predictions_train = classify(q_circuits, params, feat_vecs_train, Y_train)\n",
    "        predictions_test = classify(q_circuits, params, feat_vecs_test, Y_test)\n",
    "        acc_train = accuracy(Y_train, predictions_train)\n",
    "        acc_test = accuracy(Y_test, predictions_test)\n",
    "\n",
    "        print(\n",
    "            \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc test: {:0.7f} \"\n",
    "            \"\".format(it + 1, curr_cost.item(), acc_train, acc_test)\n",
    "        )\n",
    "\n",
    "        costs.append(curr_cost.item())\n",
    "        train_acc.append(acc_train)\n",
    "        test_acc.append(acc_test)\n",
    "\n",
    "    return costs, train_acc, test_acc\n",
    "\n",
    "\n",
    "# We now run our training algorithm and plot the results. Note that\n",
    "# for plotting, the matplotlib library is required\n",
    "\n",
    "# features, Y = load_and_process_data()\n",
    "# load()\n",
    "import numpy as nump\n",
    "\n",
    "# Y_array = nump.array(Y)\n",
    "\n",
    "costs, train_acc, test_acc = training(X, Y)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "iters = np.arange(0, total_iterations, 1)\n",
    "colors = [\"tab:red\", \"tab:blue\"]\n",
    "ax1.set_xlabel(\"Iteration\", fontsize=17)\n",
    "ax1.set_ylabel(\"Cost\", fontsize=17, color=colors[0])\n",
    "ax1.plot(iters, costs, color=colors[0], linewidth=4)\n",
    "ax1.tick_params(axis=\"y\", labelsize=14, labelcolor=colors[0])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Test Acc.\", fontsize=17, color=colors[1])\n",
    "ax2.plot(iters, test_acc, color=colors[1], linewidth=4)\n",
    "\n",
    "ax2.tick_params(axis=\"x\", labelsize=14)\n",
    "ax2.tick_params(axis=\"y\", labelsize=14, labelcolor=colors[1])\n",
    "\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
